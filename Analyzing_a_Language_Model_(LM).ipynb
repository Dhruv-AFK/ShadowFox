{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFnTd1YQwBPiGZcCOCSwvO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhruv-AFK/ShadowFox/blob/main/Analyzing_a_Language_Model_(LM).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBTLtyAwh94F",
        "outputId": "df10a655-1ed4-45fd-f81f-f2905e91b6f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model loaded successfully.\n",
            "=== Text Generation Demo ===\n",
            "Prompt: The future of artificial intelligence is\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1642: UserWarning: Unfeasible length constraints: `min_length` (56) is larger than the maximum possible length (50). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated: the future of artificial intelligence is... and the future is. in the future. the future's future is on the line. we are the world of the artificial intelligence. we're looking at artificial intelligence,'says the future\n",
            "============================================================\n",
            "Prompt: BERT models are useful because\n",
            "Generated: bert models are useful because of bert models. bert models can be useful because bert models aren't accurate. the bert model is useful because it's useful because they're useful for bert models, but that's not a good way\n",
            "============================================================\n",
            "Prompt:  \n",
            "Generated: ‚ö†Ô∏è Prompt is empty. Please provide valid text.\n",
            "============================================================\n",
            "Prompt: India's economy in the next decade will\n",
            "Generated: india's economy will soon be in the next decade. india will be in next decade to take a decade to reach a decade. the economy in india will soon see its economy in the. next 10 years. india is expected to be in\n",
            "============================================================\n",
            "Prompt: What happens if we give a super long prompt?What happens if we give a super long...\n",
            "Generated: what happens if we give a super long prompt? who gives a super quick prompt? what happens when we gave a super prompt? and how happens if. we give an. ultra long prompt... and.. if we. give a\n",
            "============================================================\n",
            "\n",
            "‚úÖ You successfully generated text using BERT2BERT with edge case handling.\n",
            "\n",
            "üîç You can now test your own prompts and analyze output quality.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# BERT for Text Generation with Attention Visualization (Google Colab)\n",
        "# ‚úÖ Updated with edge case handling\n",
        "\n",
        "# üìå Step 1: Install Required Libraries\n",
        "!pip install transformers datasets --quiet\n",
        "\n",
        "# üìå Step 2: Import Libraries\n",
        "import torch\n",
        "from transformers import BertTokenizer, EncoderDecoderModel\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# üìå Step 3: Load Pretrained BERT2BERT Model\n",
        "model_name = \"patrickvonplaten/bert2bert_cnn_daily_mail\"\n",
        "try:\n",
        "    model = EncoderDecoderModel.from_pretrained(model_name)\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    print(\"‚úÖ Model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Failed to load model:\", str(e))\n",
        "\n",
        "# üìå Step 4: Text Generation Function with Error Handling\n",
        "def generate_text(prompt, max_length=50):\n",
        "    if not prompt.strip():\n",
        "        return \"‚ö†Ô∏è Prompt is empty. Please provide valid text.\"\n",
        "\n",
        "    try:\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "        outputs = model.generate(**inputs, max_length=max_length, num_beams=4, early_stopping=True)\n",
        "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error generating text: {str(e)}\"\n",
        "\n",
        "# üìå Demo with Multiple Prompts\n",
        "print(\"=== Text Generation Demo ===\")\n",
        "input_texts = [\n",
        "    \"The future of artificial intelligence is\",\n",
        "    \"BERT models are useful because\",\n",
        "    \" \",  # Edge case: empty string\n",
        "    \"India's economy in the next decade will\",\n",
        "    \"What happens if we give a super long prompt?\" * 100  # Edge case: very long input\n",
        "]\n",
        "\n",
        "for text in input_texts:\n",
        "    print(f\"Prompt: {text[:80]}{'...' if len(text) > 80 else ''}\")\n",
        "    print(\"Generated:\", generate_text(text))\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# üìå Summary\n",
        "print(\"\"\"\n",
        "‚úÖ You successfully generated text using BERT2BERT with edge case handling.\n",
        "\n",
        "üîç You can now test your own prompts and analyze output quality.\n",
        "\"\"\")"
      ]
    }
  ]
}